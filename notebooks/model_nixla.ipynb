{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery pandas nixla pyspark\n",
    "!pip install db-dtypes\n",
    "!pip install window_ops.expanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: <insert project id>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery DB ID: <insert DB>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "BIGQUERY_DB_ID = os.getenv(\"BIGQUERY_DB_ID\")\n",
    "\n",
    "!echo \"Project ID: $PROJECT_ID\"\n",
    "!echo \"BigQuery DB ID: $BIGQUERY_DB_ID\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire les donnÃ©es depuis BigQuery dans un DataFrame Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please install the 'db-dtypes' package to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/TrendFlow/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:59\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdb_dtypes\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     date_dtype_name \u001b[38;5;241m=\u001b[39m db_dtypes\u001b[38;5;241m.\u001b[39mDateDtype\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'db_dtypes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mSELECT sale_date, product_id, quantity_sold\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mFROM `trendflow-455409.trendflow.sales_history`\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mWHERE sale_date BETWEEN \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AND \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-03-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mLIMIT 1000\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load data into a pandas dataframe\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sales_data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert date column to datetime\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sales_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msale_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(sales_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msale_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/TrendFlow/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py:2060\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1854\u001b[0m \n\u001b[1;32m   1855\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m        :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m query_result \u001b[38;5;241m=\u001b[39m wait_for_query(\u001b[38;5;28mself\u001b[39m, progress_bar_type, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m-> 2060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeography_as_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeography_as_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbool_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mint_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrange_date_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrange_date_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrange_datetime_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrange_datetime_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrange_timestamp_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrange_timestamp_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/TrendFlow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2529\u001b[0m, in \u001b[0;36mRowIterator.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[1;32m   2295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2296\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[1;32m   2316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a pandas DataFrame by loading all pages of a query.\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m \n\u001b[1;32m   2319\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2527\u001b[0m \n\u001b[1;32m   2528\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2529\u001b[0m     \u001b[43m_pandas_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_pandas_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m geography_as_object \u001b[38;5;129;01mand\u001b[39;00m shapely \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_SHAPELY_ERROR)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/TrendFlow/lib/python3.10/site-packages/google/cloud/bigquery/_pandas_helpers.py:1144\u001b[0m, in \u001b[0;36mverify_pandas_imports\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_PANDAS_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_import_exception\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m db_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_DB_TYPES_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdb_dtypes_import_exception\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Please install the 'db-dtypes' package to use this function."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import db_dtypes\n",
    "\n",
    "# Set up BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "\n",
    "# SQL query to fetch sales data\n",
    "query = \"\"\"\n",
    "SELECT sale_date, product_id, quantity_sold\n",
    "FROM `trendflow-455409.trendflow.sales_history`\n",
    "WHERE sale_date BETWEEN '2023-01-01' AND '2024-03-01'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "# Load data into a pandas dataframe\n",
    "sales_data = client.query(query).to_dataframe()\n",
    "\n",
    "# Convert date column to datetime\n",
    "sales_data['sale_date'] = pd.to_datetime(sales_data['sale_date'])\n",
    "\n",
    "# Sort the data\n",
    "sales_data = sales_data.sort_values(['product_id', 'sale_date'])\n",
    "\n",
    "# Check the first rows\n",
    "print(sales_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Forecasting Model using Nixtla MLForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from window_ops.expanding import expanding_mean\n",
    "\n",
    "# Initialize the model\n",
    "models = [RandomForestRegressor(n_estimators=100, random_state=42)]\n",
    "\n",
    "# Create an instance of MLForecast\n",
    "fcst = MLForecast(\n",
    "    models=models,\n",
    "    freq='D',  # Assuming daily frequency\n",
    "    lags=[7, 14, 30],  # Weekly, bi-weekly, and monthly lags\n",
    "    target_transforms=[Differences([1])],  # Differencing to remove trends\n",
    "    date_features=['dayofweek', 'month'],  # Add day and month as features\n",
    ")\n",
    "\n",
    "# Train the model on your dataset\n",
    "fcst.fit(df=sales_data, id_col=\"product_id\", time_col=\"sale_date\", target_col=\"quantity_sold\")\n",
    "\n",
    "print(\"â Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for 30 days ahead\n",
    "forecast_horizon = 30\n",
    "forecast_df = fcst.predict(fh=forecast_horizon)\n",
    "\n",
    "# Rename columns for clarity\n",
    "forecast_df.rename(columns={'ds': 'forecast_date', 'y_pred': 'predicted_sales'}, inplace=True)\n",
    "\n",
    "# Display some predictions\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Predictions in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define destination table\n",
    "table_id = \"your_project_id.your_dataset.sales_forecasts\"\n",
    "\n",
    "# Convert forecast date to datetime\n",
    "forecast_df['forecast_date'] = pd.to_datetime(forecast_df['forecast_date'])\n",
    "\n",
    "# Upload predictions to BigQuery\n",
    "client.load_table_from_dataframe(forecast_df, table_id).result()\n",
    "\n",
    "print(\"â Forecast results saved to BigQuery!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from airflow import DAG\n",
    "# from airflow.operators.python import PythonOperator\n",
    "# from datetime import datetime\n",
    "# from google.cloud import bigquery\n",
    "# import pandas as pd\n",
    "# from nixla import ForecastModel\n",
    "\n",
    "# def fetch_train_predict():\n",
    "#     client = bigquery.Client()\n",
    "    \n",
    "#     # RÃ©cupÃ©rer les donnÃ©es de BigQuery\n",
    "#     query = \"SELECT * FROM `my_project.my_dataset.unified_sales_data`\"\n",
    "#     df_sales = client.query(query).to_dataframe()\n",
    "    \n",
    "#     # EntraÃ®ner le modÃ¨le Nixla\n",
    "#     model = ForecastModel(model=\"NeuralProphet\")\n",
    "#     model.fit(df_sales, target=\"sales\")\n",
    "    \n",
    "#     # PrÃ©dire les ventes\n",
    "#     predictions = model.predict(future_periods=30)\n",
    "    \n",
    "#     # Stocker les prÃ©dictions dans BigQuery\n",
    "#     table_id = \"my_project.my_dataset.predictions\"\n",
    "#     job = client.load_table_from_dataframe(predictions, table_id)\n",
    "#     job.result()\n",
    "    \n",
    "#     print(\"PrÃ©dictions mises Ã  jour dans BigQuery.\")\n",
    "\n",
    "# # DÃ©finition du DAG\n",
    "# default_args = {\n",
    "#     'owner': 'airflow',\n",
    "#     'start_date': datetime(2024, 4, 1),\n",
    "#     'retries': 1\n",
    "# }\n",
    "\n",
    "# dag = DAG(\n",
    "#     dag_id='fetch_train_predict_nixla',\n",
    "#     default_args=default_args,\n",
    "#     schedule_interval='@daily'\n",
    "# )\n",
    "\n",
    "# task = PythonOperator(\n",
    "#     task_id='run_nixla_forecast',\n",
    "#     python_callable=fetch_train_predict,\n",
    "#     dag=dag\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrendFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
