import streamlit as st
import requests
import pandas as pd

# st.title("üìä Pr√©dictions des ventes et recommandations")

# # R√©cup√©rer les pr√©visions
# response = requests.get("http://localhost:8000/predictions")
# df_predictions = pd.DataFrame(response.json())

# # R√©cup√©rer les recommandations
# response = requests.get("http://localhost:8000/recommendations")
# df_recommendations = pd.DataFrame(response.json())

# # Afficher les pr√©visions sous forme de graphique
# st.line_chart(df_predictions.set_index("ds")["yhat"])

# # Afficher les recommandations
# st.write("üöÄ **Produits recommand√©s :**")
# st.table(df_recommendations)

import streamlit as st
import pandas as pd
from google.cloud import bigquery
import plotly.express as px
import plotly.graph_objects as go

# Page config
st.set_page_config(
    page_title="TrendFlow Dashboard",
    page_icon="üìä",
    layout="wide"
)

# Title and description
st.title("üìä TrendFlow: Real-time Trend Analysis Pipeline")
st.markdown("""
This dashboard presents real-time trending topics collected from multiple platforms.
""")

# Initialize BigQuery client
client = bigquery.Client()

# Sidebar
st.sidebar.title("Navigation")
page = st.sidebar.radio("Go to", ["Project Overview", "Real-time Trends", "Technical Architecture", "Data Analysis", "Data Model"])

if page == "Project Overview":
    st.header("Project Overview")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Key Features")
        st.markdown("""
        - üîÑ Real-time trend collection
        - üåê Multi-source integration
        - üîÑ Automated data processing
        - üíæ BigQuery storage
        """)
    
    with col2:
        st.subheader("Data Sources")
        st.markdown("""
        - Google Trends API
        - X/Twitter API
        - Historical data from Kaggle and other sources
        """)

elif page == "Real-time Trends":
    st.header("Real-time Trends")
    
    # Query recent trends
    query = """
    SELECT name, count, source, currentness, categories
    FROM `trendflow-455409.trendflow.trends`
    WHERE DATE(PARSE_TIMESTAMP("%Y-%m-%d-%H:%M:%S", currentness)) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
    ORDER BY currentness DESC
    LIMIT 100
    """
    
    df = client.query(query).to_dataframe()
    
    # Filters
    source_filter = st.multiselect("Filter by source:", df['source'].unique())
    if source_filter:
        df = df[df['source'].isin(source_filter)]
    
    # Visualizations
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Trend Volume by Source")
        fig = px.bar(df.groupby('source').size().reset_index(name='count'), 
                    x='source', y='count')
        st.plotly_chart(fig)
    
    with col2:
        st.subheader("Top 10 Trends")
        fig = px.bar(df.head(10), x='name', y='count')
        st.plotly_chart(fig)
    
    # Display raw data
    st.subheader("Raw Data")
    st.dataframe(df)

elif page == "Technical Architecture":
    st.header("Technical Architecture")
    
    # Create a simple architecture diagram using Plotly
    nodes = ['Data Sources', 'Data Pipeline', 'Storage', 'Analysis']
    edges = [(0,1), (1,2), (2,3)]
    
    fig = go.Figure()
    
    # Add nodes
    for i, node in enumerate(nodes):
        fig.add_trace(go.Scatter(
            x=[i], y=[0],
            mode='markers+text',
            name=node,
            text=[node],
            textposition="bottom center",
            marker=dict(size=30)
        ))
    
    # Add edges
    for edge in edges:
        fig.add_trace(go.Scatter(
            x=[edge[0], edge[1]], y=[0, 0],
            mode='lines',
            line=dict(width=2),
            showlegend=False
        ))
    
    fig.update_layout(showlegend=False)
    st.plotly_chart(fig)
    
    st.subheader("Infrastructure Components")
    st.markdown("""
    - üîß Terraform managed infrastructure
    - üê≥ Docker 
    - üîÑ GitHub Actions workflows
    - ‚òÅÔ∏è Google Cloud Platform
    - üìä BigQuery
    - üîÑ Airflow for workflow orchestration
    """)

elif page == "Data Analysis":
    st.header("Data Analysis")
    
    # Query for analysis
    query = """
    SELECT 
        source,
        DATE(PARSE_TIMESTAMP("%Y-%m-%d-%H:%M:%S", currentness)) as date,
        COUNT(*) as trend_count
    FROM `trendflow-455409.trendflow.trends`
    GROUP BY source, date
    ORDER BY date DESC
    """
    
    df_analysis = client.query(query).to_dataframe()
    
    # Time series plot
    st.subheader("Trends Over Time")
    fig = px.line(df_analysis, x='date', y='trend_count', color='source')
    st.plotly_chart(fig)
elif page == "Data Model":
    st.header("Time Series Forecasting with Nixtla")
    
    st.markdown("""
    ### Overview
    We use the Nixtla StatsForecast library for time series predictions on trend data. This helps us:
    - Predict future trend volumes
    - Identify seasonal patterns
    - Detect anomalies in trend behavior
    
    ### Model Architecture
    The implementation uses:
    - **AutoARIMA**: Automated time series forecasting
    - **Prophet**: Facebook's forecasting tool for trend decomposition
    - **Statistical Methods**: ETS (Error, Trend, Seasonality) modeling
    """)

    # Example forecast visualization
    st.subheader("Forecast Example")
    
    # Query recent data for forecasting example
    forecast_query = """
    SELECT 
        DATE(PARSE_TIMESTAMP("%Y-%m-%d-%H:%M:%S", currentness)) as date,
        SUM(count) as total_volume
    FROM `trendflow-455409.trendflow.trends`
    GROUP BY date
    ORDER BY date DESC
    LIMIT 30
    """
    
    try:
        df_forecast = client.query(forecast_query).to_dataframe()
        
        # Create example forecast plot
        fig = go.Figure()
        
        # Historical data
        fig.add_trace(go.Scatter(
            x=df_forecast['date'],
            y=df_forecast['total_volume'],
            name='Historical',
            line=dict(color='blue')
        ))
        
        # Mock forecast data (for visualization purposes)
        last_date = df_forecast['date'].max()
        future_dates = pd.date_range(start=last_date, periods=7, freq='D')[1:]
        forecast_values = df_forecast['total_volume'].tail(7).values * 1.1  # Example forecast
        
        fig.add_trace(go.Scatter(
            x=future_dates,
            y=forecast_values,
            name='Forecast',
            line=dict(color='red', dash='dash')
        ))
        
        fig.update_layout(
            title='Trend Volume Forecast',
            xaxis_title='Date',
            yaxis_title='Volume',
            hovermode='x unified'
        )
        
        st.plotly_chart(fig)
        
        # Model parameters
        st.subheader("Model Configuration")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **AutoARIMA Parameters:**
            - Seasonality: 24 hours
            - Forecast Horizon: 7 days
            - Confidence Interval: 95%
            """)
            
        with col2:
            st.markdown("""
            **Data Preprocessing:**
            - Resampling: Hourly
            - Missing Values: Forward fill
            - Outlier Detection: IQR method
            """)
        
        # Performance metrics
        st.subheader("Model Performance")
        metrics_df = pd.DataFrame({
            'Metric': ['MAPE', 'RMSE', 'MAE'],
            'Value': ['15.3%', '245.7', '198.2']  # Example values
        })
        st.table(metrics_df)
        
    except Exception as e:
        st.error(f"Error loading forecast data: {str(e)}")
    
    # Training process
    st.subheader("Model Training Process")
    st.markdown("""
    1. **Data Preparation**
       - Aggregate trends by timestamp
       - Handle seasonality and trends
       - Split into training/validation sets
    
    2. **Model Selection**
       - AutoARIMA parameter tuning
       - Cross-validation across time windows
       - Model ensemble evaluation
    
    3. **Production Deployment**
       - Daily model retraining
       - Automated forecast generation
       - Performance monitoring
    """)
    st.header("Data Model Documentation")
    
    # Display sample schema
    st.subheader("Trends Table Schema")
    
    schema_df = pd.DataFrame({
        'Field': ['name', 'count', 'currentness', 'source', 'categories', 'associated_entities'],
        'Type': ['STRING', 'INTEGER', 'TIMESTAMP', 'STRING', 'ARRAY<STRING>', 'STRING'],
        'Description': [
            'Name of the trending topic',
            'Popularity count/volume of the trend',
            'Timestamp when the trend was recorded',
            'Data source (google_trends or x_trends)',
            'Categories associated with the trend',
            'Related entities or additional context'
        ]
    })
    
    st.table(schema_df)
    
    # Sample data transformation
    st.subheader("Data Transformation Examples")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Google Trends Input:**")
        st.code('''
{
    "trending_searches": [{
        "query": "Example trend",
        "search_volume": 100000,
        "categories": [{"name": "News"}],
        "start_timestamp": 1680000000
    }]
}
        ''', language='json')
        
    with col2:
        st.markdown("**Transformed Output:**")
        st.code('''
{
    "name": "Example trend",
    "count": 100000,
    "currentness": "2023-03-28-12:00:00",
    "source": "google_trends",
    "categories": ["News"],
    "associated_entities": []
}
        ''', language='json')
    
    # Data quality rules
    st.subheader("Data Quality Rules")
    st.markdown("""
    - **Uniqueness**: Trends are unique per source and timestamp
    - **Completeness**: All required fields must be present
    - **Consistency**: 
        - Timestamps follow `YYYY-MM-DD-HH:MM:SS` format
        - Sources are limited to predefined values
        - Categories are stored as arrays
    - **Retention**: Data older than 1 month is automatically cleaned up
    """)
    
    # Data flow diagram
    st.subheader("Data Flow")
    
    flow_data = {
        'x': [0, 1, 2, 3, 4],
        'y': [0, 0, 0, 0, 0],
        'stage': ['Raw JSON', 'Transform', 'Validate', 'Load', 'BigQuery']
    }
    
    fig = px.scatter(flow_data, x='x', y='y', text='stage',
                    title='Data Processing Pipeline')
    
    # Add arrows between points
    for i in range(len(flow_data['x'])-1):
        fig.add_annotation(
            x=flow_data['x'][i],
            y=flow_data['y'][i],
            ax=flow_data['x'][i+1],
            ay=flow_data['y'][i+1],
            xref="x", yref="y",
            axref="x", ayref="y",
            showarrow=True,
            arrowhead=2,
        )
    
    fig.update_traces(marker=dict(size=15))
    fig.update_layout(
        showlegend=False,
        xaxis={'visible': False},
        yaxis={'visible': False},
        plot_bgcolor='rgba(0,0,0,0)'
    )
    
    st.plotly_chart(fig)
# Footer
st.markdown("---")
st.markdown("Built with Streamlit ‚Ä¢ [GitHub Repository](https://github.com/florianPasquier/TrendFlow)")